import time
import numpy as np
import os
import json
import requests
from requests.exceptions import HTTPError, Timeout, ConnectionError as ReqConnectionError


class Simulator:
    """
    - ì„¸ì…˜ë‹¹ ë‹¤ì¤‘ í„´ ëŒ€í™” (ê¸°ë³¸ 10í„´)
    - ë§ˆì§€ë§‰ í„´ì—ì„œë§Œ ìˆ«ì ì œì•ˆ/ëª©í‘œ ê°±ì‹  ì‹¤í–‰
    - inferred action ì œê±°, GT ê¸°ë°˜ compliance(EMA) ì‚¬ìš©
    - ì‹œë®¬ë ˆì´í„° ë‚´ë¶€ì—ì„œ ì‚¬ìš©ì ë°œí™” í”„ë¡¬í”„íŠ¸ ìƒì„±
    - Goal ë¸”ë¡ ì •ì±…:
        * ì„¸ì…˜ 0 : ì´ˆê¸° G=4.0 ê³ ì • (ê°±ì‹ X)
        * ì„¸ì…˜ 1~6 : ë§¤ ì„¸ì…˜ ê°±ì‹  í—ˆìš©(íƒìƒ‰)
        * ì„¸ì…˜ 7 ì´í›„ : 5ì„¸ì…˜ ë¸”ë¡ ìœ ì§€ (7~11, 12~16, 17~21, ...) ë¸”ë¡ ì‹œì‘ ì‹œì ì—ë§Œ ê°±ì‹  í—ˆìš©
    - ìƒì„¸ ë¡œê¹…:
        * user.respond() ë‚´ë¶€ê°’: compliance, Î¼(before/after), suggestion, noise, noise_std
        * í…ìŠ¤íŠ¸ ë¡œê·¸(simulation_log.txt) & ì„¸ì…˜ JSONì— ëª¨ë‘ ê¸°ë¡
    """

    def __init__(self, user, agent, action_space, total_steps=400, ema_alpha=0.2, turns_per_session=3):
        self.user = user
        self.agent = agent
        self.action_space = action_space
        self.total_steps = total_steps
        self.turns_per_session = max(2, int(turns_per_session))  # ìµœì†Œ 2í„´(ëŒ€í™”+ì œì•ˆ)

        self.EMA_ALPHA = ema_alpha
        self.RETRY_STATUS = {429, 500, 502, 503, 504}
        self._init_logs()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬(ì„¸ì…˜ë³„ë¡œ ì¬ì„¤ì •)
        self.conversation_history = []

        # LLM í˜¸ì¶œ ê³µí†µ ì„¸íŒ… (userì˜ ì—”ë“œí¬ì¸íŠ¸/í‚¤ ì¬ì‚¬ìš©)
        self.api_url = "https://api.openai.com/v1/chat/completions"
        self.model_name = getattr(self.user, "model_name", "gpt-5-nano")
        self.headers = getattr(self.user, "headers", {})

    # ---------- ë¡œê·¸ êµ¬ì¡° ----------
    def _init_logs(self):
        self.suggestion_trace = []
        self.ground_truth_action_trace = []
        self.reward_trace = []
        self.compliance_trace = []          # EMA (distance-based)
        self.compliance_trace_raw = []      # raw (distance-based)
        self.estimated_compliance_trace = []
        self.goal_trace = []

        # ì‚¬ìš©ì ëª¨ë¸ ê´€ì  ìƒì„¸ ë¡œê¹…(ì‹ ê·œ)
        self.user_compliance_prob_trace = []   # respond()ì˜ prob compliance
        self.behavior_mean_before_trace = []   # ì‘ë‹µ ì „ Î¼
        self.behavior_mean_after_trace = []    # ì‘ë‹µ í›„ Î¼
        self.noise_trace = []                  # ìƒ˜í”Œë§ëœ noise
        self.noise_std_trace = []              # ì‚¬ìš©ëœ noise std

        self.io_dir = "io_logs"
        os.makedirs(self.io_dir, exist_ok=True)

    def _ensure_dir(self, d):
        os.makedirs(d, exist_ok=True)
        return d

    def _save_json(self, path, data):
        self._ensure_dir(os.path.dirname(path))
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _log_io(self, session_id, turn, role, prompt_text, parsed_json, raw_text=None):
        fname = os.path.join(self.io_dir, f"session_{session_id:03}_turn_{turn:02}_{role}.json")
        payload = {"role": role, "prompt_text": prompt_text, "parsed_response": parsed_json}
        if raw_text is not None:
            payload["raw_text"] = raw_text
        self._save_json(fname, payload)

    # ---------- ê³„ì‚° ----------
    def compute_compliance(self, suggestion, gt_action):
        if suggestion is None or gt_action is None:
            return 0.0
        rng = float(np.ptp(self.action_space)) or 5.0
        comp = 1.0 - abs(float(gt_action) - float(suggestion)) / rng
        return float(np.clip(comp, 0.0, 1.0))

    def compute_reward(self, suggestion, gt_action, goal):
        """
        ì‹œê°í™”ìš© ê°„ë‹¨ ë³´ìƒ(ì •ì±…í•™ìŠµ ì œê±° ìƒíƒœì—ì„œ ê·¸ë˜í”„ ìœ ì§€ë¥¼ ìœ„í•´ ê³„ì‚°):
        - ìˆœì‘ë„(EMA ì ìš© ì „ ì›ì‹œê°’)ì— ê°€ì¤‘ì¹˜ 1.0
        - ëª©í‘œ ê·¼ì ‘ë„ exp(-(gt-goal)^2)ì— ê°€ì¤‘ì¹˜ 1.5
        """
        if gt_action is None:
            return 0.0
        goal_score = np.exp(-((gt_action - goal) ** 2))
        comp_raw = self.compute_compliance(suggestion, gt_action)
        return 1.0 * comp_raw + 1.5 * goal_score

    # ---------- ê³µí†µ LLM í˜¸ì¶œ ----------
    def _llm_json(self, model, prompt):
        payload = {"model": model, "messages": [{"role": "user", "content": prompt}], "response_format": {"type": "json_object"}}
        r = requests.post(self.api_url, json=payload, headers=self.headers, timeout=60)
        r.raise_for_status()
        content = r.json().get("choices", [{}])[0].get("message", {}).get("content", "{}").strip()
        if content.startswith("```"):
            content = content.split("```", 2)[1]
        try:
            return json.loads(content), content
        except Exception:
            return {}, content

    # ---------- ì‚¬ìš©ì ë°œí™” í”„ë¡¬í”„íŠ¸(ë‚´ì¥) ----------
    def _format_user_prompt(self, recommendation_history, action_history, history_window=5):
        """
        UserLlm.format_user_promptì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì‹œë®¬ë ˆì´í„°ì—ì„œ ì§ì ‘
        ì‚¬ìš©ì ë°œí™” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (inferred action ì—†ì´ ì•ˆì „)
        """
        def _tail_str(seq, n=history_window):
            if not seq:
                return "None"
            nums = [float(x) for x in seq if isinstance(x, (int, float))]
            tail = nums[-n:] if nums[-n:] else []
            return ", ".join(f"{v:.2f}" for v in tail) if tail else "None"

        rec_tail = _tail_str(recommendation_history, history_window)
        act_tail = _tail_str(action_history, history_window)

        profile = getattr(self.user, "user_profile", {}) or {}
        age = profile.get("age", "Unknown")
        gender = profile.get("gender", "Unknown")
        condition = profile.get("condition", "Unknown")
        mu = profile.get("mu", "Unknown")
        beta = profile.get("beta", "Unknown")
        gamma = profile.get("gamma", "Unknown")

        return f"""
        ## ğŸ§‘â€âš•ï¸ Dietary Coaching User Profile
        You are a user receiving coaching from an AI to improve your dietary habits.
        - **Your Age**: {age}
        - **Your Gender**: {gender}
        - **Your Condition**: {condition}
        - **Your typical habits (Î¼)** are described as: {mu}
        - **You tend to be (Î²)**: {beta}
        - **You are (Î³)**: {gamma} to external influences.
        ### ğŸ“ˆ Recent Context
        - Recent agent recommendations (last {history_window}): [{rec_tail}]
        - My actual actions (last {history_window}): [{act_tail}]
        ### ğŸ¯ Your Task
        React to the agent's latest message in a natural, conversational way consistent with your profile. 
        Keep it short (1â€“2 sentences). Do not propose numbers.
        ### âœï¸ Output Instructions
        Return strict JSON with only the 'utterance' key:
        {{"utterance": "What you would say out loud to the coach."}}
        """.strip()

    def _compliance_summary(self, window: int = 10) -> dict:
        vals = [v for v in self.compliance_trace if v is not None]
        recent = vals[-window:] if vals else []
        to_float = (lambda x: None if x is None else float(x))
        return {
            "count": len(vals),
            "last": to_float(vals[-1]) if vals else None,
            "mean": (float(np.mean(vals)) if vals else None),
            "recent_mean": (float(np.mean(recent)) if recent else None),
            "estimated_by_agent": to_float(getattr(self.agent, "estimated_compliance", None))
        }

    # ---------- ì„¸ì…˜ ----------
    def run_session(self, session_id: int, first_session: bool = False):
        self.conversation_history = []
        session_log = []

        # ---- Goal ì¡°ì • ì •ì±… ----
        # ì„¸ì…˜ 0: ì´ˆê¸° Goal=4.0 ê³ ì • (ê°±ì‹ X)
        if session_id == 0:
            self.agent.goal_update_allowed = False
        # ì„¸ì…˜ 1~6: ììœ  ì¡°ì •
        elif 1 <= session_id <= 6:
            self.agent.goal_update_allowed = True
        # ì„¸ì…˜ 7 ì´í›„: 5ì„¸ì…˜ ë¸”ë¡ ìœ ì§€, ë¸”ë¡ ì‹œì‘ì—ì„œë§Œ í—ˆìš©
        else:
            # ë¸”ë¡ ì‹œì‘ ì„¸ì…˜ = 7, 12, 17, ...  => (session_id - 7) % 5 == 0
            self.agent.goal_update_allowed = ((session_id - 7) % 5 == 0)

        # ì¤‘ê°„ í„´ë“¤: ì½”ì¹­ ëŒ€í™”
        for t in range(1, self.turns_per_session):
            # Agent ëŒ€í™” í„´
            self.agent.run_context = {
                "session_id": session_id,
                "current_turn": t,
                "max_turns": self.turns_per_session,
                "total_sessions": self.total_steps,
                "compliance_summary": self._compliance_summary()
            }
            agent_msg = self.agent.coach_turn(self.conversation_history)
            self.conversation_history.append({"role": "assistant", "content": agent_msg})

            # User ì‘ë‹µ í„´
            user_prompt = self._format_user_prompt(
                recommendation_history=self.suggestion_trace,
                action_history=self.ground_truth_action_trace
            )
            user_json, raw = self._llm_json(self.model_name, user_prompt)
            user_msg = user_json.get("utterance", "Okay.")
            self.conversation_history.append({"role": "user", "content": user_msg})

            session_log.append({
                "turn": t,
                "agent_utterance": agent_msg,
                "user_utterance": user_msg
            })

        # ë§ˆì§€ë§‰ í„´: ìˆ«ì ì œì•ˆ(ë° ë¸”ë¡ ì •ì±…ì— ë”°ë¼ ëª©í‘œ ê°±ì‹  ê°€ëŠ¥)
        self.agent.run_context = {
            "session_id": session_id,
            "current_turn": self.turns_per_session,
            "max_turns": self.turns_per_session,
            "total_sessions": self.total_steps,
            "compliance_summary": self._compliance_summary()
        }
        suggestion, plan_msg = self.agent.ask_llm_for_plan(self.conversation_history, self.ground_truth_action_trace)
        self.agent.suggestion_history.append(suggestion)

        # ëŒ€í™”ì— ë°˜ì˜
        self.conversation_history.append({"role": "assistant", "content": plan_msg})
        # ì‚¬ìš©ì ìì—°ì–´ ì‘ë‹µ(ì„ íƒì â€”ë¡œê·¸ ì¼ê´€ì„± ìœ„í•´ ìœ ì§€)
        user_prompt = self._format_user_prompt(
            recommendation_history=self.suggestion_trace + [suggestion],
            action_history=self.ground_truth_action_trace
        )
        user_json, _ = self._llm_json(self.model_name, user_prompt)
        user_msg = user_json.get("utterance", "I'll try.")
        self.conversation_history.append({"role": "user", "content": user_msg})

        session_log.append({
            "turn": self.turns_per_session,
            "agent_utterance": plan_msg,
            "user_utterance": user_msg,
            "suggestion": suggestion
        })

        # ì‚¬ìš©ì GT í–‰ë™ ì‚°ì¶œ + ìƒì„¸ ë‚´ë¶€ê°’ ìˆ˜ì§‘
        resp = self.user.respond(suggestion, return_details=True)
        if isinstance(resp, tuple):
            gt_action, u = resp
        else:
            # (í˜¸í™˜ìš©) ì˜ˆì™¸ì ìœ¼ë¡œ ìƒì„¸ê°’ì´ ì—†ì„ ë•Œ
            gt_action, u = resp, {
                "compliance": None,
                "behavior_mean_before": None,
                "behavior_mean_after": None,
                "suggestion": suggestion,
                "noise": None,
                "noise_std": None
            }

        # distance ê¸°ë°˜ compliance + EMA
        comp_raw = self.compute_compliance(suggestion, gt_action)
        comp_ema = comp_raw if not self.compliance_trace else \
            (self.EMA_ALPHA * comp_raw + (1 - self.EMA_ALPHA) * self.compliance_trace[-1])

        # ë³´ìƒ(ì‹œê°í™”ìš©)
        reward = self.compute_reward(suggestion, gt_action, self.agent.goal_behavior)

        # --- íŠ¸ë ˆì´ìŠ¤ ì ì¬ (ì‹ ê·œ + ê¸°ì¡´) ---
        self.suggestion_trace.append(suggestion)
        self.ground_truth_action_trace.append(gt_action)
        self.compliance_trace_raw.append(comp_raw)
        self.compliance_trace.append(comp_ema)
        self.reward_trace.append(reward)
        self.goal_trace.append(self.agent.goal_behavior)

        # ì‚¬ìš©ì ëª¨ë¸ ê´€ì  ìƒì„¸ ë¡œê¹…
        self.user_compliance_prob_trace.append(u.get("compliance"))
        self.behavior_mean_before_trace.append(u.get("behavior_mean_before"))
        self.behavior_mean_after_trace.append(u.get("behavior_mean_after"))
        self.noise_trace.append(u.get("noise"))
        self.noise_std_trace.append(u.get("noise_std"))

        # ì—ì´ì „íŠ¸ ì‚¬í›„ ì—…ë°ì´íŠ¸
        self.agent.after_session_update(comp_ema)
        self.estimated_compliance_trace.append(self.agent.estimated_compliance)

        # ì„¸ì…˜ ì €ì¥(JSON)
        if session_log:
            session_log[-1].update({
                "ground_truth_action": gt_action,
                "compliance_raw": comp_raw,
                "compliance_ema": comp_ema,
                "goal": self.agent.goal_behavior,
                "reward": reward,
                # ìƒì„¸ í•„ë“œ ì¶”ê°€
                "user_compliance_prob": u.get("compliance"),
                "behavior_mean_before": u.get("behavior_mean_before"),
                "behavior_mean_after": u.get("behavior_mean_after"),
                "noise": u.get("noise"),
                "noise_std": u.get("noise_std")
            })

        self._save_session_log(session_log, session_id, first_session)
        return session_log

    def _save_session_log(self, session_log, session_id, first_session):
        os.makedirs("sessions", exist_ok=True)
        path = f"sessions/{{'profile' if first_session else 'session'}}_{session_id:03}.json"
        self._save_json(path, session_log)

    # ---------- ì „ì²´ ë£¨í”„ ----------
    def train(self):
        # Session 0: ì´ˆê¸° ëª©í‘œ 4.0 ê³ ì •
        self.run_session(session_id=0, first_session=True)
        # ì´í›„ ì„¸ì…˜: ë¸”ë¡ ì •ì±…ì— ë”°ë¼ goal ê°±ì‹  í—ˆìš©/ì°¨ë‹¨
        for session_id in range(1, self.total_steps):
            self.run_session(session_id=session_id, first_session=False)
        self.save_log()

    def save_log(self, filename="simulation_log.txt"):
        with open(filename, "w") as f:
            # ê¸°ì¡´ í—¤ë” + ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€
            f.write(
                "Step\tSuggestion\tGTAction\t"
                "ComplianceRaw\tComplianceEMA\tReward\tGoal\t"
                "UserComplianceProb\tMuBefore\tMuAfter\tNoise\tNoiseStd\n"
            )
            for i in range(len(self.suggestion_trace)):
                def fmt(val):
                    return f"{val:.4f}" if (val is not None and not (isinstance(val, float) and np.isnan(val))) else "NA"
                f.write(
                    f"{i+1}\t"
                    f"{fmt(self.suggestion_trace[i])}\t"
                    f"{fmt(self.ground_truth_action_trace[i])}\t"
                    f"{fmt(self.compliance_trace_raw[i])}\t"
                    f"{fmt(self.compliance_trace[i])}\t"
                    f"{fmt(self.reward_trace[i])}\t"
                    f"{fmt(self.goal_trace[i])}\t"
                    f"{fmt(self.user_compliance_prob_trace[i] if i < len(self.user_compliance_prob_trace) else None)}\t"
                    f"{fmt(self.behavior_mean_before_trace[i] if i < len(self.behavior_mean_before_trace) else None)}\t"
                    f"{fmt(self.behavior_mean_after_trace[i] if i < len(self.behavior_mean_after_trace) else None)}\t"
                    f"{fmt(self.noise_trace[i] if i < len(self.noise_trace) else None)}\t"
                    f"{fmt(self.noise_std_trace[i] if i < len(self.noise_std_trace) else None)}\n"
                )
